{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import zipfile as zp\n",
    "from functools import reduce\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to keep.\n",
    "usecols = [\n",
    "    'OPR_DT',\n",
    "    'OPR_HR',\n",
    "    'Timestamp',\n",
    "    'NODE_ID',\n",
    "    'MARKET_RUN_ID',\n",
    "    'LMP_TYPE',\n",
    "    'XML_DATA_ITEM',\n",
    "    'MW'\n",
    "]\n",
    "\n",
    "# Date strings for formatting URLs.\n",
    "month_dict = {\n",
    "    'Jan': ['0101T08:00-0000', '0201T08:00-0000'],\n",
    "    'Feb': ['0201T08:00-0000', '0301T08:00-0000'],\n",
    "    'Mar': ['0301T08:00-0000', '0401T07:00-0000'],\n",
    "    'Apr': ['0401T07:00-0000', '0501T07:00-0000'],\n",
    "    'May': ['0501T07:00-0000', '0601T07:00-0000'],\n",
    "    'Jun': ['0601T07:00-0000', '0701T07:00-0000'],\n",
    "    'Jul': ['0701T07:00-0000', '0801T07:00-0000'],\n",
    "    'Aug': ['0801T07:00-0000', '0901T07:00-0000'],\n",
    "    'Sep': ['0901T07:00-0000', '1001T07:00-0000'],\n",
    "    'Oct': ['1001T07:00-0000', '1101T07:00-0000'],\n",
    "    'Nov': ['1101T07:00-0000', '1201T08:00-0000'],\n",
    "    'Dec': ['1201T08:00-0000', '0101T08:00-0000']\n",
    "}\n",
    "\n",
    "# TH (Trading Hub) nodes.\n",
    "th_node_list = [\n",
    "    'TH_NP15_GEN-APND',\n",
    "    'TH_SP15_GEN-APND',\n",
    "    'TH_ZP26_GEN-APND'\n",
    "]\n",
    "\n",
    "# Default LAP (Load Aggregation Point) nodes corresponding to utility territories.\n",
    "lap_node_list = [\n",
    "    'DLAP_PGAE-APND',\n",
    "    'DLAP_SCE-APND',\n",
    "    'DLAP_SDGE-APND',\n",
    "    'DLAP_VEA-APND'\n",
    "]\n",
    "\n",
    "# Sub-LAP nodes.\n",
    "sub_lap_node_list = [\n",
    "    'SLAP_PGCC-APND', 'SLAP_PGEB-APND', 'SLAP_PGF1-APND', 'SLAP_PGFG-APND', 'SLAP_PGHB-APND', 'SLAP_PGKN-APND', 'SLAP_PGLP-APND', 'SLAP_PGNB-APND', 'SLAP_PGNC-APND', 'SLAP_PGNP-APND', 'SLAP_PGNV-APND', 'SLAP_PGP2-APND', 'SLAP_PGSA-APND', 'SLAP_PGSB-APND', 'SLAP_PGSF-APND', 'SLAP_PGSI-APND', 'SLAP_PGSN-APND', 'SLAP_PGST-APND', 'SLAP_PGZP-APND',\n",
    "    'SLAP_SCEC-APND', 'SLAP_SCEN-APND', 'SLAP_SCEW-APND', 'SLAP_SCHD-APND', 'SLAP_SCLD-APND', 'SLAP_SCNW-APND', \n",
    "    'SLAP_SDG1-APND', \n",
    "    'SLAP_VEA-APND'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hourly_data(year, nodes):\n",
    "    '''Downloads df from Oasis Portal by month for a year of choice and aggregates into single CSV file.'''\n",
    "\n",
    "    node_entry = reduce(lambda x, y: x +','+ y, nodes)\n",
    "    name_entry = reduce(lambda x, y: x +', '+ y, nodes)\n",
    "\n",
    "    # Directory path.\n",
    "    dir = f'/Users/parkerwild/GitHub/ca_nem/data/CAISO_LMPs/{str(year)}/{name_entry}'\n",
    "\n",
    "    print('Accessing Chrome driver...')\n",
    "\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    prefs = {'download.default_directory': f'{dir}/ZIPs'}\n",
    "    chrome_options.add_experimental_option('prefs', prefs)\n",
    "    \n",
    "    driver = webdriver.Chrome(\n",
    "        options=chrome_options, \n",
    "        executable_path='/Users/parkerwild/GitHub/ca_nem/chromedriver_mac64/chromedriver.exe'\n",
    "    )\n",
    "\n",
    "    for month in month_dict.keys():\n",
    "\n",
    "        # Handle last day of year.\n",
    "        start_year = year\n",
    "        end_year = year\n",
    "\n",
    "        if month == 'Dec':\n",
    "            end_year += 1\n",
    "\n",
    "        # URL.\n",
    "        api_call = 'http://oasis.caiso.com/oasisapi/SingleZip?queryname=PRC_LMP&resultformat=6&' + \\\n",
    "            'startdatetime=' + str(start_year) + month_dict[month][0] + '&' + \\\n",
    "            'enddatetime=' + str(end_year) + month_dict[month][1] + '&' + \\\n",
    "            'version=1&market_run_id=DAM&node=' + node_entry\n",
    "        \n",
    "        print(f'Downloading data for {month}...')\n",
    "\n",
    "        # Request.\n",
    "        driver.get(api_call)\n",
    "\n",
    "        # Sleep.\n",
    "        time.sleep(15)\n",
    "\n",
    "    print('Closing Chrome driver...')\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_hourly_data(dir):\n",
    "    '''Unzip files and concatenate CSVs.'''\n",
    "\n",
    "    print('Unzipping files...')\n",
    "        \n",
    "    zip_files = glob.glob(f'{dir}/ZIPs/*.zip')\n",
    "\n",
    "    for zip_filename in zip_files:\n",
    "\n",
    "        zip_handler = zp.ZipFile(zip_filename, \"r\")\n",
    "        zip_handler.extractall(f'{dir}/CSVs')\n",
    "\n",
    "    print('Concatenating CSVs...')\n",
    "\n",
    "    csv_files = glob.glob(f'{dir}/CSVs/*.csv')\n",
    "\n",
    "    csvs = []\n",
    "\n",
    "    for csv in csv_files:\n",
    "\n",
    "        data = pd.read_csv(csv)\n",
    "        csvs.append(data)\n",
    "\n",
    "    df = pd.concat(csvs)\n",
    "\n",
    "    print('Cleaning data...')\n",
    "\n",
    "    # Adjust hour column to [0, 23].\n",
    "    df['OPR_HR'] = df['OPR_HR'].astype(int) - 1\n",
    "\n",
    "    # Add extra hour on Mar 13th due to DST.\n",
    "    extra_hour = df.loc[(df['OPR_DT'] == '2022-03-13') & (df['OPR_HR'] == 1)].copy()\n",
    "    extra_hour['OPR_HR'] = 2\n",
    "    df = pd.concat([df, extra_hour])\n",
    "\n",
    "    # Skip extra hour on Nov 6th due to DST.\n",
    "    df = df.loc[df['OPR_HR'] < 24]\n",
    "\n",
    "    # Convert to datetime.\n",
    "    df['Timestamp'] = pd.to_datetime(df['OPR_DT'] + ' ' + df['OPR_HR'].astype(str) + ':00:00')\n",
    "\n",
    "    # Drop duplicates, if necessary (but shouldn't be...)\n",
    "    df.drop_duplicates(subset=['Timestamp', 'NODE_ID', 'LMP_TYPE'], inplace=True, ignore_index=True)\n",
    "    \n",
    "    # Sort by interval start time.\n",
    "    df.sort_values(by=['Timestamp', 'NODE_ID', 'LMP_TYPE'], inplace=True, ignore_index=True)\n",
    "\n",
    "    # Keep subset of columns.\n",
    "    df = df[usecols]\n",
    "\n",
    "    print('Writing to CSV...')\n",
    "\n",
    "    # Write dataframe to CSV.    \n",
    "    df.to_csv(f'{dir}/Aggregated_LMPs.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing Chrome driver...\n",
      "Downloading data for Jan...\n",
      "Downloading data for Feb...\n",
      "Downloading data for Mar...\n",
      "Downloading data for Apr...\n",
      "Downloading data for May...\n",
      "Downloading data for Jun...\n",
      "Downloading data for Jul...\n",
      "Downloading data for Aug...\n",
      "Downloading data for Sep...\n",
      "Downloading data for Oct...\n",
      "Downloading data for Nov...\n",
      "Downloading data for Dec...\n",
      "Closing Chrome driver...\n"
     ]
    }
   ],
   "source": [
    "# Extract data.\n",
    "dir = extract_hourly_data(2022, ['DLAP_PGAE-APND', 'DLAP_SCE-APND', 'DLAP_SDGE-APND'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping files...\n",
      "Concatenating CSVs...\n",
      "Cleaning data...\n",
      "Writing to CSV...\n"
     ]
    }
   ],
   "source": [
    "# Process data.\n",
    "df = combine_hourly_data(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8760"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm the year includes 8760 hours.\n",
    "len(df['Timestamp'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify problematic days, if necessary.\n",
    "for dt in df['OPR_DT'].unique():\n",
    "    if len(df.loc[df['OPR_DT'] == dt, 'OPR_HR'].unique()) != 24:\n",
    "        print(dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
